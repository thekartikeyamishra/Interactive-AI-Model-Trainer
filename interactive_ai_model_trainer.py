# -*- coding: utf-8 -*-
"""Interactive AI Model Trainer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M7cOH7amlPpCBOef0582kI7S4b54nBw5
"""

from IPython.display import Markdown

display(Markdown("""
# üß™ Basic Interactive AI Model Trainer

This notebook provides a very simple, interactive way to understand the basic workflow of training a Machine Learning model.
It is designed for **educational purposes** to illustrate fundamental concepts.

**What you can do here:**
1.  Upload your own data in CSV format.
2.  Select features and a target variable from your data.
3.  Choose a simple classification model.
4.  Train the model on your data.
5.  See basic performance metrics.

**‚ö†Ô∏è Important Limitations:**
* This tool uses very simple models from the `scikit-learn` library, suitable for basic structured data.
* It is **NOT** designed for complex tasks like image/video processing, deepfake detection, or creating advanced AI.
* The goal is to learn the foundational steps of a machine learning pipeline, not to build state-of-the-art AI.

Concerns about AI misuse, such as deepfakes and misinformation, are valid and are being addressed by the broader research community through more specialized and advanced techniques. This notebook is a humble first step in understanding how AI models learn.
"""))

print("Installing/updating libraries...")
!pip install -q scikit-learn pandas numpy matplotlib seaborn ipywidgets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer # To handle missing values

import ipywidgets as widgets
from ipywidgets import interact, interactive, fixed, IntSlider, VBox, HBox, Layout
from IPython.display import display, clear_output

print("\nLibraries ready!")

#@title 1. Introduction: A Basic Interactive AI Model Trainer
from IPython.display import Markdown, display

display(Markdown("""
# üß™ Basic Interactive AI Model Trainer (v2 - Fixed & with Dummy Data)

This notebook provides a very simple, interactive way to understand the basic workflow of training a Machine Learning model.
It is designed for **educational purposes** to illustrate fundamental concepts.

**What you can do here:**
1.  Upload your own data in CSV format OR generate simple dummy data.
2.  Select features and a target variable from your data.
3.  Choose a simple classification model.
4.  Train the model on your data.
5.  See basic performance metrics.

**‚ö†Ô∏è Important Limitations:**
* This tool uses very simple models from the `scikit-learn` library, suitable for basic structured data.
* It is **NOT** designed for complex tasks like image/video processing, deepfake detection, or creating advanced AI.
* The goal is to learn the foundational steps of a machine learning pipeline, not to build state-of-the-art AI.

Concerns about AI misuse, such as deepfakes and misinformation, are valid and are being addressed by the broader research community through more specialized and advanced techniques. This notebook is a humble first step in understanding how AI models learn.
"""))
#@title 2. Install and Import Libraries
# Most of these are often pre-installed in Colab, but explicit install is good practice.
print("Installing/updating libraries...")
!pip install -q scikit-learn pandas numpy matplotlib seaborn ipywidgets

# Import libraries after installation
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC # Added SVM as another option
from sklearn.ensemble import RandomForestClassifier # Added Random Forest
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer # To handle missing values
from sklearn.datasets import make_classification # For dummy data

import ipywidgets as widgets
from ipywidgets import interact, interactive, fixed, IntSlider, VBox, HBox, Layout
from IPython.display import display, clear_output
import io # For handling byte stream from file upload

print("\nLibraries ready!")
#@title 3. Configure Your Model & Data (UI)

# --- Global variable to store dataframe ---
df_global = None
output_area = widgets.Output() # Main output area for messages and results

# --- UI Element Definitions ---
style = {'description_width': 'initial'}
l_50width = Layout(width='50%')
l_95width = Layout(width='95%')

# 1. Data Source Selection
data_source_toggle = widgets.ToggleButtons(
    options=['Upload CSV', 'Use Dummy Data'],
    description='Data Source:',
    disabled=False,
    button_style='', # 'success', 'info', 'warning', 'danger' or ''
    tooltips=['Upload your own CSV file', 'Generate a simple classification dataset'],
    style=style
)

# 2. File Uploader for CSV (initially hidden if dummy data is default)
file_uploader = widgets.FileUpload(
    accept='.csv',
    multiple=False,
    description='Upload CSV Data:',
    style=style,
    layout=Layout(display='none') # Initially hidden
)

# 3. Button to Load/Generate Data
load_generate_button = widgets.Button(
    description='Load/Generate Data',
    button_style='primary',
    tooltip='Click to load uploaded CSV or generate dummy data',
    icon='database',
    layout=Layout(width='auto', margin='10px 0 0 0')
)

# 4. Data Preview Area
data_preview_output = widgets.Output(layout=Layout(height='200px', overflow_y='auto', border='1px solid lightgray', margin='5px 0 0 0'))

# 5. Column Selection (will be populated after load/generation)
feature_columns_selector = widgets.SelectMultiple(
    description='Select Feature Columns:', disabled=True, style=style, rows=5, layout=l_50width)
target_column_selector = widgets.Dropdown(
    description='Select Target Column:', disabled=True, style=style, layout=l_50width)

# 6. Model Selection (FIXED HERE)
# Define model instances first
log_reg_model = LogisticRegression(solver='liblinear', random_state=42, max_iter=200)
dt_model = DecisionTreeClassifier(random_state=42)
knn_model = KNeighborsClassifier()
svc_model = SVC(random_state=42, probability=True) # probability=True for predict_proba if needed later
rf_model = RandomForestClassifier(random_state=42)

model_options_list = [
    ('Logistic Regression', log_reg_model),
    ('Decision Tree Classifier', dt_model),
    ('K-Nearest Neighbors (KNN)', knn_model),
    ('Support Vector Machine (SVM)', svc_model),
    ('Random Forest Classifier', rf_model)
]

model_selector = widgets.Dropdown(
    options=model_options_list,
    value=log_reg_model, # Set value to one of the instances from the options
    description='Choose AI Model:',
    style=style,
    layout=l_50width
)

# 7. Train/Test Split Ratio
test_size_slider = widgets.FloatSlider(
    value=0.2, min=0.1, max=0.5, step=0.05,
    description='Test Set Size Ratio:', style=style, layout=l_50width,
    readout_format='.2f'
)

# 8. Preprocessing Options
scaler_checkbox = widgets.Checkbox(
    value=True, description='Scale Numerical Features (StandardScaler)', style=style, indent=False)


# 9. Button to Trigger Training
train_button = widgets.Button(
    description='Preprocess Data & Train Model!',
    button_style='success',
    tooltip='Click to start preprocessing and training',
    icon='cogs',
    layout=Layout(width='auto', margin='10px 0 0 0'),
    disabled=True # Disabled until data is loaded
)

# --- Event Handlers (Callbacks) ---
def on_data_source_toggle_change(change):
    if change['new'] == 'Upload CSV':
        file_uploader.layout.display = 'flex'
    else: # 'Use Dummy Data'
        file_uploader.layout.display = 'none'

data_source_toggle.observe(on_data_source_toggle_change, names='value')
# Initialize visibility based on default toggle state
on_data_source_toggle_change({'new': data_source_toggle.value})


def on_load_generate_button_clicked(b):
    global df_global
    with output_area:
        clear_output(wait=True)
        display(Markdown("--- Loading/Generating Data ---"))
        train_button.disabled = True # Disable train button until data is ready

    source_type = data_source_toggle.value

    if source_type == 'Upload CSV':
        if not file_uploader.value: # Check if a file has been selected
            with output_area:
                clear_output(wait=True)
                print("‚ö†Ô∏è Please select a CSV file to upload first.")
            return

        uploaded_file_info = file_uploader.value
        filename = list(uploaded_file_info.keys())[0]
        content = uploaded_file_info[filename]['content']
        try:
            df_global = pd.read_csv(io.BytesIO(content))
            status_message = f"üëç Successfully loaded '{filename}'. Shape: {df_global.shape}."
        except Exception as e:
            df_global = None
            status_message = f"‚ùå Error loading or parsing CSV: {e}"
            with data_preview_output: clear_output(wait=True) # Clear preview on error
            feature_columns_selector.options = []
            target_column_selector.options = []
            feature_columns_selector.disabled = True
            target_column_selector.disabled = True

    elif source_type == 'Use Dummy Data':
        try:
            X_dummy, y_dummy = make_classification(
                n_samples=200, n_features=5, n_informative=3, n_redundant=1,
                n_classes=2, random_state=42, flip_y=0.05
            )
            df_global = pd.DataFrame(X_dummy, columns=[f'feature_{i+1}' for i in range(X_dummy.shape[1])])
            df_global['target'] = y_dummy
            status_message = f"üëç Successfully generated dummy data. Shape: {df_global.shape}."
        except Exception as e:
            df_global = None
            status_message = f"‚ùå Error generating dummy data: {e}"
            with data_preview_output: clear_output(wait=True)
            feature_columns_selector.options = []
            target_column_selector.options = []
            feature_columns_selector.disabled = True
            target_column_selector.disabled = True

    with output_area: # Display status message
        clear_output(wait=True)
        print(status_message)

    if df_global is not None:
        all_columns = df_global.columns.tolist()
        feature_columns_selector.options = all_columns
        target_column_selector.options = all_columns
        if all_columns: # Set default target if possible
            target_column_selector.value = all_columns[-1] if 'target' not in all_columns else 'target'
        feature_columns_selector.disabled = False
        target_column_selector.disabled = False
        train_button.disabled = False # Enable train button

        with data_preview_output:
            clear_output(wait=True)
            print("Data Preview (first 5 rows):")
            display(df_global.head())
    else: # If df_global is None after attempts
        with data_preview_output:
            clear_output(wait=True)
            print("No data available for preview.")


load_generate_button.on_click(on_load_generate_button_clicked)

# --- Display UI ---
ui_title = widgets.HTML("<h2>ü§ñ Simple AI Model Training Configuration (v2)</h2>")
data_source_section = VBox([
    widgets.HTML("<h4>1. Choose Data Source & Load/Generate</h4>"),
    data_source_toggle,
    file_uploader, # Visibility controlled by toggle
    load_generate_button,
    data_preview_output
])
column_config_section = VBox([
    widgets.HTML("<h4>2. Configure Features & Target</h4>"),
    widgets.HTML("<p>Select the columns from your data to be used as input features (X) and the single column to be predicted (y).</p>"),
    HBox([feature_columns_selector, target_column_selector])
])
model_config_section = VBox([
    widgets.HTML("<h4>3. Configure Model & Training</h4>"),
    model_selector,
    test_size_slider,
    scaler_checkbox
])

display(ui_title)
display(data_source_section)
display(column_config_section)
display(model_config_section)
display(train_button)
display(widgets.HTML("<hr><h2>üìä Results & Output:</h2>")) # Separator for results
display(output_area) # This is where messages and results will go

def on_train_button_clicked(b):
    global df_global
    with output_area: # All output from this function will go here
        clear_output(wait=True) # Clear previous results
        display(Markdown("--- Starting Model Training Process ---"))

        if df_global is None:
            print("‚ùå Error: No data loaded/generated. Please use Step 1 first.")
            return

        selected_features = list(feature_columns_selector.value)
        selected_target = target_column_selector.value

        if not selected_features or not selected_target:
            print("‚ùå Error: Please select feature columns and a target column.")
            return
        if selected_target in selected_features:
            print(f"‚ùå Error: Target column '{selected_target}' cannot also be a feature column.")
            return

        print(f"Selected Features: {selected_features}")
        print(f"Selected Target: {selected_target}")

        # --- 1. Prepare Data (X, y) ---
        try:
            X_raw = df_global[selected_features].copy()
            y_raw = df_global[selected_target].copy()
        except KeyError as e:
            print(f"‚ùå Error: One or more selected columns not found in the DataFrame: {e}")
            print("This might happen if the data changed after selection. Please re-load/re-generate data and re-select columns.")
            return


        # --- 2. Preprocessing ---
        print("\n--- Preprocessing Data ---")

        # Handle Missing Values (Simple Imputation)
        # For numerical features
        num_cols = X_raw.select_dtypes(include=np.number).columns
        if not X_raw[num_cols].empty and not X_raw[num_cols].isnull().sum().sum() == 0: # Check if any NaNs exist
            print("Handling missing numerical values using mean imputation...")
            num_imputer = SimpleImputer(strategy='mean')
            X_raw[num_cols] = num_imputer.fit_transform(X_raw[num_cols])
        elif not X_raw[num_cols].empty:
            print("No missing numerical values found in features.")
        else:
            print("No numerical columns found in selected features.")


        # For categorical features (if any selected - simple mode imputation)
        cat_cols = X_raw.select_dtypes(include='object').columns
        if not X_raw[cat_cols].empty and not X_raw[cat_cols].isnull().sum().sum() == 0:
            print("Handling missing categorical values using most frequent imputation...")
            cat_imputer = SimpleImputer(strategy='most_frequent')
            X_raw[cat_cols] = cat_imputer.fit_transform(X_raw[cat_cols])
        elif not X_raw[cat_cols].empty:
            print("No missing categorical values found in categorical features.")
        else:
            print("No categorical columns found in selected features.")


        # Encode Categorical Features (Simple One-Hot Encoding for simplicity here)
        if len(cat_cols) > 0:
            print(f"One-hot encoding categorical features: {list(cat_cols)}")
            X_processed = pd.get_dummies(X_raw, columns=cat_cols, drop_first=True, dummy_na=False) # dummy_na=False to not create NaN columns
        else:
            X_processed = X_raw
            print("No categorical features to encode, or all features are numerical.")


        # Encode Target Variable (if it's categorical)
        le = LabelEncoder()
        if y_raw.dtype == 'object' or pd.api.types.is_categorical_dtype(y_raw):
            print(f"Encoding target variable '{selected_target}' using LabelEncoder.")
            y_processed = le.fit_transform(y_raw)
            target_classes_map = dict(zip(le.classes_, le.transform(le.classes_)))
            print(f"Target classes mapping: {target_classes_map}")
        else:
            # Assuming it's already numerical for classification (e.g., 0, 1) or regression
            y_processed = y_raw.copy() # Ensure it's a copy
            target_classes_map = {label: label for label in np.unique(y_processed)} # Create a simple map for consistency
            print(f"Target variable '{selected_target}' is already numerical. Unique values: {np.unique(y_processed)}")


        # Scale Numerical Features (if checkbox is ticked)
        if scaler_checkbox.value:
            current_num_cols = X_processed.select_dtypes(include=np.number).columns
            if len(current_num_cols) > 0:
                print("Scaling numerical features using StandardScaler...")
                scaler = StandardScaler()
                X_processed[current_num_cols] = scaler.fit_transform(X_processed[current_num_cols])
            else:
                print("No numerical features found to scale after one-hot encoding.")
        else:
            print("Skipping numerical feature scaling.")

        display(Markdown("Preview of Processed Features (X) Head:"))
        display(X_processed.head())
        print(f"Processed X shape: {X_processed.shape}")
        print(f"Processed y shape: {y_processed.shape}")

        if X_processed.empty:
            print("‚ùå Error: Processed features (X) are empty. Check your feature selection and data.")
            return

        # --- 3. Split Data ---
        test_ratio = test_size_slider.value
        print(f"\nSplitting data into training ({1-test_ratio:.0%}) and testing ({test_ratio:.0%}) sets...")
        try:
            # Stratify only if there are at least 2 samples per class after processing
            unique_y, counts_y = np.unique(y_processed, return_counts=True)
            can_stratify = len(unique_y) > 1 and all(c >= 2 for c in counts_y)

            if can_stratify:
                X_train, X_test, y_train, y_test = train_test_split(
                    X_processed, y_processed, test_size=test_ratio, random_state=42, stratify=y_processed
                )
                print("Used stratified splitting.")
            else:
                print("Cannot stratify (e.g., single class or insufficient samples per class). Using regular split.")
                X_train, X_test, y_train, y_test = train_test_split(
                    X_processed, y_processed, test_size=test_ratio, random_state=42
                )
        except Exception as e:
            print(f"‚ùå Error during data splitting: {e}")
            print("Attempting split without stratification as a fallback...")
            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    X_processed, y_processed, test_size=test_ratio, random_state=42
                )
            except Exception as e_nostrat:
                 print(f"‚ùå Error during data splitting even without stratification: {e_nostrat}")
                 return

        print(f"Training data shape: X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"Testing data shape: X_test: {X_test.shape}, y_test: {y_test.shape}")

        if X_train.empty or X_test.empty:
            print("‚ùå Error: Training or testing set is empty after split. Check data size and test ratio.")
            return

        # --- 4. Train Model ---
        # The model_selector.value directly gives the instantiated model
        model_name_display = [name for name, mod_instance in model_options_list if mod_instance == model_selector.value][0]
        model = model_selector.value # Use the instance from the dropdown

        print(f"\n--- Training Model: {model_name_display} ---")
        try:
            model.fit(X_train, y_train)
            print("‚úÖ Model training complete.")
        except Exception as e:
            print(f"‚ùå Error during model training: {e}")
            print("Please check your data, feature selection, and preprocessing steps.")
            print("Common issues: incompatible data types, all NaN columns, insufficient samples, etc.")
            return


        # --- 5. Evaluate Model ---
        print("\n--- Evaluating Model ---")
        try:
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)

            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)

            print(f"üéØ Training Accuracy: {train_accuracy:.4f}")
            print(f"üéØ Test Accuracy: {test_accuracy:.4f}")

            print("\nClassification Report (Test Set):")
            # Use the inverse_transform of LabelEncoder if it was used, or sorted unique values from y_processed
            if 'le' in locals() and hasattr(le, 'classes_'):
                class_labels_for_report = [str(c) for c in le.classes_]
            else: # Numerical target
                class_labels_for_report = [str(c) for c in sorted(np.unique(y_processed))]


            report = classification_report(y_test, y_pred_test, target_names=class_labels_for_report, zero_division=0)
            print(report)

            print("\nConfusion Matrix (Test Set):")
            cm = confusion_matrix(y_test, y_pred_test, labels=np.unique(np.concatenate((y_test, y_pred_test)))) # Ensure all labels are considered
            plt.figure(figsize=(max(6, len(class_labels_for_report)*1.5), max(4, len(class_labels_for_report)*1)))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=class_labels_for_report if len(class_labels_for_report) == len(np.unique(np.concatenate((y_test, y_pred_test)))) else 'auto',
                        yticklabels=class_labels_for_report if len(class_labels_for_report) == len(np.unique(np.concatenate((y_test, y_pred_test)))) else 'auto')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.title('Confusion Matrix')
            plt.show()

        except Exception as e:
            print(f"‚ùå Error during model evaluation: {e}")
            print("This can happen if the model failed to train properly or if there are issues with the test data (e.g. unseen labels).")

        print("\n--- Process Finished ---")

# Attach the handler to the button
train_button.on_click(on_train_button_clicked)

